services:
  # Main application service
  ollama-smart-proxy:
    build: .
    image: ollama-smart-proxy:latest
    container_name: proxy
    ports:
      - "11555:11555"
    environment:
      - OLLAMA_PROXY_SERVER_HOST=0.0.0.0
      - OLLAMA_PROXY_SERVER_PORT=11555
      - OLLAMA_PROXY_REDIS_URL=redis://redis:6379/0
      - OLLAMA_PROXY_OLLAMA_HOST=ollama
      # RAG Plugin Configuration
      - LIGHTRAG_HOST=http://lightrag:9621
      - SEARXNG_HOST=http://searxng:8080
    env_file:
      - .env.lightrag
    depends_on:
      redis:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      postgres:
        condition: service_healthy
      searxng:
        condition: service_started
      lightrag:
        condition: service_started
    networks:
      - app-network
    volumes:
      - ./config.json:/app/config.json
      - ./src/plugins:/app/src/plugins
      - ./.env.lightrag:/app/.env.lightrag

  # LightRAG service
  lightrag:
    container_name: lightrag
    build:
      context: ./LightRAG
      dockerfile: Dockerfile
    ports:
      - "${PORT:-9621}:9621"
    volumes:
      - ./data/rag_storage:/app/data/rag_storage
      - ./data/inputs:/app/data/inputs
      - ./LightRAG/config.ini:/app/config.ini
      - ./.env.lightrag:/app/.env
    env_file:
      - ./.env.lightrag
    environment:
      - LLM_BINDING=ollama
      - LLM_MODEL=qwen2.5-coder:1.5b
      - LLM_BINDING_HOST=http://ollama:11434
      - OLLAMA_LLM_NUM_CTX=32768
      - EMBEDDING_BINDING=ollama
      - EMBEDDING_MODEL=nomic-embed-text:latest
      - EMBEDDING_BINDING_HOST=http://ollama:11434
      - EMBEDDING_DIM=768
    depends_on:
      redis:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      postgres:
        condition: service_healthy
      ollama:
        condition: service_healthy
    restart: unless-stopped
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - app-network

  # Redis service
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  # Neo4j service
  neo4j:
    image: neo4j:5-community
    container_name: neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/password
      - NEO4J_dbms_security_auth__enabled=false
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - app-network

  # PostgreSQL with vector extensions
  postgres:
    image: pgvector/pgvector:pg17
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=rag_db
      - POSTGRES_USER=ollama_proxy
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-postgres.sql:/docker-entrypoint-initdb.d/init-postgres.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ollama_proxy -d rag_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  # pgAdmin service
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    ports:
      - "5050:80"
    environment:
      - PGADMIN_DEFAULT_EMAIL=pgadmin4@pgadmin.org
      - PGADMIN_DEFAULT_PASSWORD=admin
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - app-network

  # Searxng service
  searxng:
    image: searxng/searxng:latest
    container_name: searxng
    ports:
      - "8080:8080"
    volumes:
      - searxng_data:/etc/searxng
    environment:
      - SEARXNG_BASE_URL=http://localhost:8080/
    entrypoint:
      - sh
      - -c
      - |
        # 1) Create the patch file
        cat >/tmp/settings.yml <<'EOF'
        use_default_settings: true
        server:
          secret_key: "generaterandomkeyhere"
        search:
          formats:
            - html
            - json
        engines:
          - name: duckduckgo
            engine: duckduckgo
            shortcut: ddg
          - name: google
            engine: google
            shortcut: gg
          - name: wikipedia
            engine: wikipedia
            shortcut: wp
          - name: bing
            engine: bing
            shortcut: bi
        EOF

        # 2) Point SearXNG to our temporary config
        export SEARXNG_SETTINGS_PATH="/tmp/settings.yml"

        # 3) Run the image entrypoint (path differs by tag)
        if [ -x "/usr/local/searxng/entrypoint.sh" ]; then
          exec /usr/local/searxng/entrypoint.sh
        elif [ -x "/usr/local/searxng/dockerfiles/docker-entrypoint.sh" ]; then
          exec /usr/local/searxng/dockerfiles/docker-entrypoint.sh
        else
          echo "Critical Error: Could not find SearXNG entrypoint."
          echo "Checked: /usr/local/searxng/entrypoint.sh and /usr/local/searxng/dockerfiles/docker-entrypoint.sh"
          exit 1
        fi

    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - app-network

  # Redis Insight service
  redisinsight:
    image: redislabs/redisinsight:latest
    container_name: redisinsight
    ports:
      - "5540:5540"
    environment:
      - RI_GLOBAL_REDIS_HOST=redis
      - RI_GLOBAL_REDIS_PORT=6379
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - app-network

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-proxy
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # Enable NVIDIA GPU access for CUDA support
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    runtime: nvidia  # Required for GPU access in Docker
    command: serve
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  open-webui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: open-webui-proxy
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      - ollama
    volumes:
      - open-webui_data:/app/backend/data
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  redis_data:
  neo4j_data:
  neo4j_logs:
  neo4j_import:
  postgres_data:
  searxng_data:
  pgadmin_data:
  ollama_data:
  open-webui_data:
